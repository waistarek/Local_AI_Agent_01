# ğŸ• Local_AI_Agent_01

Ein **lokaler KIâ€‘Agent**, der Fragen zu Restaurantbewertungen beantwortet.  
Das Projekt nutzt **LangChain**, **Ollama** und **Chroma**, um eine **lokale RAGâ€‘Pipeline** (Retrievalâ€‘Augmented Generation) zu implementieren â€“ ohne Cloud.

---

## ğŸ§© Ãœberblick

- **Use Case:** Stelle Fragen wie â€Wie ist die Pizza bewertet?â€œ oder â€Was bemÃ¤ngeln GÃ¤ste am Service?â€œ  
- **Datenquelle:** Lokale CSV-Datei mit Bewertungen (wird **nicht** ins Repo eingecheckt).  
- **Ablauf:** CSV â†’ Embeddings â†’ Vektorspeicher (Chroma) â†’ semantische Suche â†’ Antwort durch LLM.

---

## ğŸ§  Architektur (kurz)

- `vector.py` lÃ¤dt die CSV, erzeugt Embeddings (`mxbai-embed-large` Ã¼ber Ollama) und speichert sie in **Chroma**.
- `main.py` baut eine einfache Prompt-Kette mit **LangChain** und einem lokalen LLM (z.â€¯B. `llama3.2` Ã¼ber Ollama).  
  FÃ¼r jede Nutzerfrage werden passende Reviews abgerufen und in die Antwort eingebunden.

---

## ğŸ“¦ Anforderungen

- **Python** 3.10+
- **Ollama** installiert und laufend (https://ollama.com/)  
  Modelle, die benÃ¶tigt werden:
  ```bash
  ollama pull llama3.2
  ollama pull mxbai-embed-large
  ```
- AbhÃ¤ngigkeiten aus `requirements.txt`:
  - `langchain`
  - `langchain-ollama`
  - `langchain-chroma`
  - `pandas`

> ğŸ’¡ Tipp: Erstelle eine virtuelle Umgebung, bevor du installierst.

---

## ğŸ—‚ Projektstruktur

```
Local_AI_Agent_01/
â”œâ”€ main.py                      # Interaktives Q&A im Terminal
â”œâ”€ vector.py                    # CSV einlesen, Embeddings, Chroma-Retriever
â”œâ”€ requirements.txt             # Python-AbhÃ¤ngigkeiten
â””â”€ reviews.csv   # 
```

---

## ğŸ§° Installation & Setup

1. **Repo klonen**
   ```bash
   git clone https://github.com/waistarek/Local_AI_Agent_01.git
   cd Local_AI_Agent_01
   ```

2. **Virtuelle Umgebung**
   ```bash
   python -m venv .venv
   # macOS/Linux
   source .venv/bin/activate
   # Windows (PowerShell)
   .venv\Scripts\Activate.ps1
   ```

3. **AbhÃ¤ngigkeiten installieren**
   ```bash
   pip install -r requirements.txt
   ```

4. **Ollama & Modelle vorbereiten**
   ```bash
   ollama pull llama3.2
   ollama pull mxbai-embed-large
   ```

5. **CSV-Datei lokal hinzufÃ¼gen**  
   Lege `reviews.csv` ins Projektverzeichnis.  
   **Wichtig:** Die Datei wird **nicht** ins Git-Repo hochgeladen (siehe `.gitignore` unten).

---

## ğŸ§¾ Datenschema (CSV)

Erwartete Spaltennamen (Beispiele in `vector.py`):

- **Title** â€“ Kurztitel der Bewertung  
- **Review** â€“ Volltext der Bewertung  
- **Rating** â€“ Numerische Bewertung (z.â€¯B. 1â€“5)  
- **Date** â€“ Datum der Bewertung (z.â€¯B. `2024-05-10`)

> Achte auf exakte Spaltennamen. Abweichungen fÃ¼hren zu Fehlern beim Einlesen.

---

## â–¶ï¸ AusfÃ¼hren

Starte zuerst Ollama im Hintergrund (falls nicht automatisch).  
Dann im Projektordner:

```bash
python main.py
```

Du siehst eine Eingabeaufforderung wie:
```
#######################################################
Ask your question (q to quit): Welche Gerichte werden oft gelobt?
```

- Beenden mit: `q`  
- Stelle Fragen auf Deutsch oder Englisch.

---

## ğŸ› ï¸ Konfiguration & Ablage des Vektorspeichers

StandardmÃ¤ÃŸig wird Chroma lokal gespeichert.  
Wenn du einen **persistenten Pfad** erzwingen mÃ¶chtest, ergÃ¤nze in `vector.py` beim Erstellen des Chroma-Stores einen `persist_directory` (z.â€¯B. `./chroma_langchain_database`) und rufe nach dem EinfÃ¼gen `persist()` auf:

```python
vector_store = Chroma(
    collection_name="restaurant_reviews",
    embedding_function=embeddings,
    persist_directory="./chroma_langchain_database",
)
# nach dem HinzufÃ¼gen:
vector_store.persist()
```

> Dadurch bleiben die Embeddings auch nach Skriptneustarts erhalten.

---

## ğŸš« Wichtiger Git-Hinweis (Dateien nicht hochladen)

Bitte **keine DatenblÃ¤tter** committen:
- CSV- und Excel-Dateien aus Datenschutz-/GrÃ¼nden der Repo-Hygiene ausschlieÃŸen.

Beispiel fÃ¼r `.gitignore` (fÃ¼ge diese Datei im Repo hinzu):
```
# Daten
*.csv
*.xlsx
*.xls
realistic_restaurant_reviews.csv

# Vektordatenbanken / temporÃ¤re Artefakte
chroma*
*.chroma
*.db
**/__pycache__/
.venv/
```

---

## ğŸ§ª Beispiel-Fragen

- â€Was sagen GÃ¤ste Ã¼ber die Pizza Margherita?â€œ  
- â€Welche Kritikpunkte kommen am hÃ¤ufigsten vor?â€œ  
- â€Wie ist die Stimmung insgesamt im Juni 2024?â€œ

---

## â— Fehlerbehebung (FAQ)

- **`ModuleNotFoundError`**: AbhÃ¤ngigkeiten mit `pip install -r requirements.txt` installieren.  
- **Ollama-Fehler / Modell nicht gefunden**: `ollama pull <modellname>` ausfÃ¼hren und prÃ¼fen, dass der Ollama-Dienst lÃ¤uft.  
- **Leere Antworten**: PrÃ¼fe, ob die CSV korrekt benannt ist und die Spalten wie oben heiÃŸen.  
- **Persistenz funktioniert nicht**: `persist_directory` setzen und nach dem HinzufÃ¼gen `persist()` aufrufen (siehe oben).

---

## ğŸ—ºï¸ Roadmap / Verbesserungen

- Eingabedaten validieren (z.â€¯B. fehlende Spalten erkennen).  
- Web-GUI (z.â€¯B. Streamlit) statt Terminal.  
- Mehr Metriken / Filter (z.â€¯B. nach Datum oder Rating).  
- Unit-Tests und Typannotationen.  

> Hinweis zu `vector.py`: Achte darauf, dass `metadata` den SchlÃ¼ssel `rating` korrekt schreiben sollte.

---

## ğŸ“œ Lizenz

MIT-Lizenz â€“ gerne forken, verbessern und teilen.

---

## ğŸ‘¤ Autor

Tarek Wais  Â· Â© 2025
